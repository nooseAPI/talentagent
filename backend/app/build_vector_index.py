# backend/app/build_vector_index.py

import os
from pathlib import Path
import pandas as pd

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================

# í˜„ì¬ íŒŒì¼: backend/app/build_vector_index.py
# BASE_DIR = backend
BASE_DIR = Path(__file__).resolve().parent.parent

# ì—‘ì…€ ìœ„ì¹˜
DATA_FILE = BASE_DIR / "data" / "pmo_docs" / "TALENT_AX_Sample.xlsx"

# ì¸ë±ìŠ¤ ì €ì¥ ìœ„ì¹˜
INDEX_DIR = BASE_DIR / "app" / "vector_index"
INDEX_DIR.mkdir(parents=True, exist_ok=True)

# =========================
# ì—‘ì…€ â†’ í…ìŠ¤íŠ¸ ë³€í™˜
# =========================

def load_excel_documents(excel_path: Path) -> list[str]:
    """ì—‘ì…€ì„ ì½ì–´ì„œ ê° í–‰ì„ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ í•©ì¹œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    if not excel_path.exists():
        raise FileNotFoundError(f"ì—‘ì…€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {excel_path}")

    df = pd.read_excel(excel_path)

    texts: list[str] = []
    for _, row in df.iterrows():
        row_text = " | ".join(
            f"{col}: {row[col]}" for col in df.columns
        )
        texts.append(row_text)

    return texts


# =========================
# Vector Index ìƒì„±
# =========================

def build_vector_index() -> None:
    print("ğŸ“„ Excel íŒŒì¼ ê²½ë¡œ:", DATA_FILE)

    texts = load_excel_documents(DATA_FILE)
    print(f"âœ… ì—‘ì…€ ë¡œë“œ ì™„ë£Œ (row ìˆ˜: {len(texts)})")

    # í…ìŠ¤íŠ¸ chunk ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
    )

    chunks: list[str] = []
    for t in texts:
        chunks.extend(splitter.split_text(t))

    print(f"âœ‚ï¸ ë¶„í• ëœ chunk ê°œìˆ˜: {len(chunks)}")

    docs = [
        Document(
            page_content=chunk,
            metadata={"source": DATA_FILE.name},
        )
        for chunk in chunks
    ]

    # =========================
    # Ollama Embeddings
    # =========================

    embeddings = OllamaEmbeddings(
        model="nomic-embed-text",
        base_url="http://localhost:11434",  # ê¸°ë³¸ê°’ (ëª…ì‹œ ê¶Œì¥)
    )

    print("ğŸ§  Ollama ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    print("EMBEDDINGS TYPE:", type(embeddings))
    db = FAISS.from_documents(docs, embeddings)
    db.save_local(INDEX_DIR)

    
    print("\nâœ… VECTOR INDEX ìƒì„± ì™„ë£Œ!")
    print("ğŸ“ ì €ì¥ ìœ„ì¹˜:", INDEX_DIR.resolve())
    print("   - index.faiss")
    print("   - index.pkl")


if __name__ == "__main__":
    build_vector_index()
