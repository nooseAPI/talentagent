import os
from pathlib import Path
from typing import List

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS

# =========================
# ê²½ë¡œ ì„¤ì •
# =========================

BASE_DIR = Path(__file__).resolve().parent
VECTOR_PATH = BASE_DIR / "vector_index"
VECTOR_PATH.mkdir(exist_ok=True)

# =========================
# Ollama Embeddings (ì „ì—­)
# =========================

embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)

# =========================
# ì‹ ê·œ Vector Index ìƒì„±
# =========================

def build_vector_index(texts: List[str], sources: List[str]):
    documents = []

    for t, s in zip(texts, sources):
        documents.append(
            Document(
                page_content=t,
                metadata={"source": s}
            )
        )

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100
    )

    chunks = splitter.split_documents(documents)

    print("EMBEDDINGS TYPE:", type(embeddings))
    print("ğŸ“ VECTOR INDEX DIR:", str(VECTOR_PATH))
    db = FAISS.from_documents(chunks, embeddings)
    db.save_local(str(VECTOR_PATH))

    print("âœ… vector_index ìµœì´ˆ ìƒì„± ì™„ë£Œ")


# =========================
# Vector Index ìƒì„± or ì—…ë°ì´íŠ¸
# =========================

def build_or_update_vector_index(texts: List[str]):
    faiss_file = VECTOR_PATH / "index.faiss"

    if faiss_file.exists():
        print("ğŸ” ê¸°ì¡´ vector_index ë¡œë“œ í›„ ì—…ë°ì´íŠ¸")
        db = FAISS.load_local(
            str(VECTOR_PATH),
            embeddings,
            allow_dangerous_deserialization=True
        )
        db.add_texts(texts)
    else:
        print("ğŸ†• ì‹ ê·œ vector_index ìƒì„±")
        db = FAISS.from_texts(texts, embeddings)

    db.save_local(str(VECTOR_PATH))
    print("âœ… vector_index ì €ì¥ ì™„ë£Œ")
